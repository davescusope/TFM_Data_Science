# Data Science Master Final Project
## Competition Margin Forecasting in Regulated Electricity Markets

### 1. Summary
The electric market in Spain is an open market, which has a part regulated by semi-private bodies for its management. These organizations are REE for the technical part and OMIE for the operations part.

In this regulated framework, data on electricity prices, electricity percentage of each technology, total volume of production, and so on are published daily on their respective webpages.

- https://www.ree.es/es/estadisticas-del-sistema-electrico-espanol
- http://www.omie.es/inicio

It should be noted that one of the rules of this regulated market is the sale price of electricity.
It could be thought that each technology is sold at a different price, but in reality OMEL performs an "adjustment" exercise of the price for which all technologies are paid at the same price, regardless of the generation cost of each one.

This mechanism is a real time one and quite complex so we will not go deeper about it. what we will do is staying with the important thing : *Every technology ispaid at the same price, but have different margin "revenue - costs"*

Understanding this starting point, defining better or worse the margin of a company is to be able to define better or worse production costs.

In addition, one more factor must be taken into account. The different electricity generation companies in Spain have the same technologies. This, together with a non-monopolized market, makes the purchase prices of services or materials very similar for all companies.

That is why if we know the costs associated with electricity for one technology and a specific period of time for a particular company, they can be extrapolated to all other companies.
Only we must know what are the productions (MWH) of the target company.
 <br/><br/>
 <br/><br/>
### 2. Previous Considerations

The aim of this project is double:

**On one hand**, the objective is to explore the widest range possible of techniques learnt in this Master applied to a real jobcase for a data scientist.
Instead of starting from a predefined point where all the data is already gathered and prepared, we are gonna search , explore and flow through the transformatios needed to reach that very far point which which is being ready to train a machine learning model, using in this journey different languages, libraries and programs.

*That is, take advantage of the TFM to review and deepen on what has been learned*
 <br/><br/>

**On the other hand**, due to my daily job I am on a great possition to compare all the tools learnt on this masters degree with 
the ones I'm more used to in my job ( SQL & SAS ).
That's why I decided to duplicate ( at least in part) one project I have already developed using that previous mentioned tools, already knowing the destination, but not the journey.

That second objective will be accomplished and condensed in the last part of the TFM *Conclusions*



For this second point, and even when most of the data used in this project are available to be downloaded directly from the REE oficial webpage  "https://www.ree.es/es/estadisticas-del-sistema-electrico-espanol", I've prefered to use internal infomation after having been anonymized.

For the anonymization purpose, there were used different procedures depending on the data-origin, but basically it is just a replacement based on SQL joins.
Examples of the process can be watch on the links bellow if desired

- https://drive.google.com/open?id=10AUcEVijW7kJpaKWIv6xQTRc7dHBdTYb
- https://drive.google.com/open?id=1acEfWbsbnwtrR_uR5v-J6XW0zwtYpJOj
 <br/><br/>
 <br/><br/>
### 3. How to Run

1. Create a folder wherever you want and create inside it two new folders named: "Imports" and "Outputs" respectively

2. Download all the required files and put them into the "Imports" folder
```
ORIGIN_COSTS_E4E
ORIGIN_MARKET_LIQUIDATIONS
ORIGIN_NC_WASTES_201712
ORIGIN_NC_WASTES_201812
```
3. Download from this github the files and put them into the root folder of the #1
```
01.Liquidaciones_mercado 
02. 
03. 
04. 
05. 
06. 
07. 
```   
4. Execute files from number 1 to 7 in order

5. Finally, execute "Visualization1.twbx" 
 <br/><br/>
 <br/><br/>
### 4. Project Structure
This TFM has been structured following the linear approach shown below

![Information flow2](https://user-images.githubusercontent.com/46086706/58645501-c719ad00-8303-11e9-8b8d-eace8c3a5ea6.jpg)


This approach follows the previously highlighted idea (objective #1) that I wanted to apply as much different tools as possible (keeping the common sense) for achieving the objective.

That is why the first part was developed with R while the second one is based on Python and the visualization part was developed with Tableau.

Probably, it would have been easier to do everything with one single technology, but taking into account all the time that was going to devote to the TFM, it was a magnificent opportunity to force myself to use more technologies.

 <br/><br/>
 <br/><br/>
### 5. Code Development

#### R
I have used three modules for data import and processing, each one of them attacks a different file because of its peculiarities in terms of data structure. 
Each module can be executed separately since they are not dependent on each other. For information, with my computer, the execution times of the three modules were in order:

```
Module 01.Liquidaciones_Mercado <<<<<Time difference of 2.439813 hours>>>>>>
Module 02.Residuos_Nucleares    <<<<<Time difference of 6.325827 secs>>>>>>
Module 03.SAP_DATA              <<<<<Time difference of 1.927194 mins>>>>>>
```
The core of R processing used in this project is the library Dplyr and its pipelines were used to facilitate transformations, but there were used others like stringr for string treatment, lubridate for dates treatmen and so on

It is important to say that the whole code is commented so it can be follow in detail by executing and scrutinizing it but to sum up, the highlights of this section were:

* Module 01 is used to import and process a file too large for my computer, so it forced me to develope a function which part and process each chunk of the file. 
After this process, all chuncks are put together again and the new dataframe is processed to fit in the common structure for the three modules output.

* Module 02 is composed of two different files so they were imported, binded and processed as a whole

* Module 03 module three comes directly from accounting information and it's structure is by monthly acumulated information. that means that the value from March 2018, is in fact the value of January + February + March.
For the nature of this project, we need to process it by monthly differences in order to get the single monthly value.

#### Python


#### Visualization: Tableau


 <br/><br/>
 <br/><br/>
### 7. Conclusions

As said before, through the different phases of this project I could made an approach to the comparation between the different technologies I have used to build the integral margin of the company One.
Thanks to this situation, I will make the evaluation of the new ones vs the old ones based on 5 points.

Criteria are simple: 
- Every point has the same weight for the final mark ( eventhough we all know that not all of them are the same)
- I will evaluate them ONLY for the application on this project, and not for it's possibilities out of this frame
 <br/><br/>
 
| ASPECT                      |      Python & R      |      SAS & SQL       |
|-----------------------------|:--------------------:|:--------------------:|
| Prize                       |         5            |           1          |
| Learning Rate               |         3            |           4          |
| Grouth & Industrialization  |         3            |           5          |
| Query Ease                  |         3            |           5          |
| Groups and filters          |         3            |           5          |
| Data transformation         |         3            |           5          |
| Statistical Models          |         5            |           2          |
| Visualization               |         5            |           1          |
| Community & Support         |         5            |           3          |
|                             |                      |                      |
| **Total**                   |       35/45          |         31/45        |

 <br/><br/>
**Finally, I will expose the personal conclusions I got from this Master's degree from a whole point of view of the current data science situation out of the frame of this project:**

- &#9745; New techniques of analysis objectibly overcome the old ones as an average
- &#9745; New techniques starts from a disadvantageous point due to the fact that the old ones are already consolidated
- &#9745; New techniques are freeware. Point of inflexion from the previous model of private licenses and payments
- &#9745; New techniques are open source, fact that expose them to an exponential growth
- &#9745; State of the art is now condensed on the new techniques
 <br/><br/>
 <br/><br/>
### 8. About the author

David Escuredo Sopeña

https://es.linkedin.com/in/david-escuredo-sopeña-43679789
