# Data Science Master Final Project
## Competition Margin Forecasting in Regulated Electricity Markets

### 1. Summary
The electric market in Spain is an open market, which has a part regulated by semi-private bodies for its management. These organizations are REE for the technical part and OMIE for the operations part.

In this regulated framework, data on electricity prices, electricity percentage of each technology, total volume of production, and so on are published daily on their respective webpages.

- https://www.ree.es/es/estadisticas-del-sistema-electrico-espanol
- http://www.omie.es/inicio

It should be noted that one of the rules of this regulated market is the sale price of electricity.
It could be thought that each technology is sold at a different price, but in reality OMEL performs an "adjustment" exercise of the price for which all technologies are paid at the same price, regardless of the generation cost of each one.

This mechanism is a real time one and quite complex so we will not go deeper about it. what we will do is staying with the important thing : *Every technology ispaid at the same price, but have different margin "revenue - costs"*

Understanding this starting point, defining better or worse the margin of a company is to be able to define better or worse production costs.

In addition, one more factor must be taken into account. The different electricity generation companies in Spain have the same technologies. This, together with a non-monopolized market, makes the purchase prices of services or materials very similar for all companies.

That is why if we know the costs associated with electricity for one technology and a specific period of time for a particular company, they can be extrapolated to all other companies.
Only we must know what are the productions (MWH) of the target company.
 <br/><br/>
 <br/><br/>
### 2. Previous Considerations

The aim of this project is double:

**On one hand**, the objective is to explore the widest range possible of techniques learnt in this Master applied to a real jobcase for a data scientist.
Instead of starting from a predefined point where all the data is already gathered and prepared, we are gonna search , explore and flow through the transformatios needed to reach that very far point which which is being ready to train a machine learning model, using in this journey different languages, libraries and programs.

*That is, take advantage of the TFM to review and deepen on what has been learned*
 <br/><br/>

**On the other hand**, due to my daily job I am on a great possition to compare all the tools learnt on this masters degree with 
the ones I'm more used to in my job ( SQL & SAS ).
That's why I decided to duplicate ( at least in part) one project I have already developed using that previous mentioned tools, already knowing the destination, but not the journey.

That second objective will be accomplished and condensed in the last part of the TFM *Conclusions*



For this second point, and even when most of the data used in this project are available to be downloaded directly from the REE oficial webpage  "https://www.ree.es/es/estadisticas-del-sistema-electrico-espanol", I've prefered to use internal infomation after having been anonymized.

For the anonymization purpose, there were used different procedures depending on the data-origin, but basically it is just a replacement based on SQL joins.
Examples of the process can be watch on the links bellow if desired

- https://drive.google.com/open?id=10AUcEVijW7kJpaKWIv6xQTRc7dHBdTYb
- https://drive.google.com/open?id=1acEfWbsbnwtrR_uR5v-J6XW0zwtYpJOj
<br/><br/>
### 3.How to Run

1. Create a folder wherever you want and create inside it two new folders named: "Imports" and "Outputs" respectively

2. Download all the required files
   * ORIGIN_COSTS_E4E, ORIGIN_MARKET_LIQUIDATIONS, ORIGIN_NC_WASTES_201712, ORIGIN_NC_WASTES_201812
   
3. Download from this github the files and drop them into the root folder of the #1
   * 01.Liquidaciones_mercado 02. 03. 04. 05. 06. 07. 
   
4. Execute files from number 1 to 7 in order

5. Finally, execute "Visualization1.twbx" 
 <br/><br/>
 <br/><br/>
### 3. Project Structure
This TFM has been structured following two different approaches:

![Information flow2](https://user-images.githubusercontent.com/46086706/58645501-c719ad00-8303-11e9-8b8d-eace8c3a5ea6.jpg)


The first one is the real data flow and how I have been developing it.
THe second one is the general structure of this document, where you can see the main points of it.
 <br/><br/>
 <br/><br/>
### 7. Conclusions

As said before, through the different phases of this project I could made an approach to the comparation between the different technologies I have used to build the integral margin of the company One.
Thanks to this situation, I will make the evaluation of the new ones vs the old ones based on 10 points.

Criteria are simple: 
- Every point has the same weight for the final mark ( eventhough we all know that not all of them are the same)
- I will evaluate them ONLY for the application on this project, and not for it's possibilities out of this frame



<center>

| ASPECT                      |      Python & R      |      SAS & SQL       |
|-----------------------------|:--------------------:|:--------------------:|
| Prize                       |         5            |           1          |
| Learning Rate               |         3            |           4          |
| Grouth & Industrialization  |         3            |           5          |
| Muscle to move information  |         3            |           5          |
| Query Ease                  |         3            |           5          |
| Groups and filters          |         3            |           5          |
| Data transformation         |         3            |           5          |
| Statistical Models          |         5            |           2          |
| Visualization               |         5            |           1          |
| Community & Support         |         5            |           3          |
|                             |                      |                      |
| **Total**                   |       38/50          |         36/50        |

</center>



**Finally, I will expose the personal conclusions I got from this Master's degree from a whole point of view of the current data science situation out of the frame of this project:**

- New techniques of analysis objectibly overcome the old ones as an average
- New techniques starts from a very disadvantageous point due to the fact that the old ones are already consolidated
- New techniques are freeware. Point of inflexion from the previous model of private licenses and payments
- New techniques are open source, fact that expose them to an exponential growth
- State of the art is now condensed on the new techniques
 <br/><br/>
 <br/><br/>
### 8. About the author

David Escuredo Sopeña
https://es.linkedin.com/in/david-escuredo-sopeña-43679789
